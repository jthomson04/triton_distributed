# Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.

# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.

# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

# 70B

# Prefill
# python /app/tensorrt_llm/examples/quantization/quantize.py --model_dir /llama3-70b-hf/ --dtype float16 --qformat fp8 --kv_cache_dtype fp8 --output_dir /tmp/tllm_checkpoint --calib_size 512 --tp_size {2,4}
# trtllm-build --checkpoint_dir /tmp/tllm_checkpoint --output_dir /data/70B-TRT/ --gpt_attention_plugin float16 --workers 4 --max_batch_size 128 --max_seq_len 8192 --max_num_tokens 8192 --use_fused_mlp --reduce_fusion enable --multiple_profiles enable

# Generation
# python /app/tensorrt_llm/examples/quantization/quantize.py --model_dir /llama3-70b-hf/ --dtype float16 --qformat fp8 --kv_cache_dtype fp8 --output_dir /tmp/tllm_checkpoint --calib_size 512 --tp_size {2,4}
# trtllm-build --checkpoint_dir /tmp/tllm_checkpoint --output_dir /data/70B-TRT/ --gpt_attention_plugin float16 --workers 4 --max_batch_size 128 --max_seq_len 1024 --max_num_tokens 128 --use_fused_mlp --reduce_fusion enable --multiple_profiles enable

# Baseline
# python /app/tensorrt_llm/examples/quantization/quantize.py --model_dir /llama3-70b-hf/ --dtype float16 --qformat fp8 --kv_cache_dtype fp8 --output_dir /tmp/tllm_checkpoint --calib_size 512 --tp_size 8
# trtllm-build --checkpoint_dir /tmp/tllm_checkpoint --output_dir /data/70B-TRT/ --gpt_attention_plugin float16 --workers 8 --max_batch_size 512 --max_seq_len 8192 --max_num_tokens 16384 --use_fused_mlp --reduce_fusion enable --multiple_profiles enable


# 8B

# Prefill
# python /app/tensorrt_llm/examples/quantization/quantize.py --model_dir /llama3-8b-hf/ --dtype float16 --qformat fp8 --kv_cache_dtype fp8 --output_dir /tmp/tllm_checkpoint --calib_size 512 --tp_size 1
# trtllm-build --checkpoint_dir /tmp/tllm_checkpoint --output_dir /data/8B-TRT/ --gpt_attention_plugin float16 --workers 1 --max_batch_size 256 --max_seq_len 8192 --max_num_tokens 8192 --use_fused_mlp --multiple_profiles enable

# Generation
# python /app/tensorrt_llm/examples/quantization/quantize.py --model_dir /llama3-70b-hf/ --dtype float16 --qformat fp8 --kv_cache_dtype fp8 --output_dir /tmp/tllm_checkpoint --calib_size 512 --tp_size 1
# trtllm-build --checkpoint_dir /tmp/tllm_checkpoint --output_dir /data/8B-TRT/ --gpt_attention_plugin float16 --workers 1 --max_batch_size 256 --max_seq_len 1024 --max_num_tokens 256 --use_fused_mlp --multiple_profiles enable

# Baseline
# python /app/tensorrt_llm/examples/quantization/quantize.py --model_dir /llama3-8b-hf/ --dtype float16 --qformat fp8 --kv_cache_dtype fp8 --output_dir /tmp/tllm_checkpoint --calib_size 512 --tp_size 2
# trtllm-build --checkpoint_dir /tmp/tllm_checkpoint --output_dir /data/8B-TRT/ --gpt_attention_plugin float16 --workers 2 --max_batch_size 512 --max_seq_len 8192 --max_num_tokens 16384 --use_fused_mlp --reduce_fusion enable --multiple_profiles enable


KNOWN_MODELS = {
    "mock": {
        "hf_id": "meta-llama/Meta-Llama-3.1-8B-Instruct",
        "download_patterns": ["*.json"],
        "max_num_tokens": 2048,
        "max_batch_size": 512,
        "templates": [
            "preprocessing",
            "postprocessing",
            "ensemble",
            (
                "/workspace/examples/llm/tensorrtllm/operators/triton_core_models/mock",
                "context",
            ),
            (
                "/workspace/examples/llm/tensorrtllm/operators/triton_core_models/mock",
                "generate",
            ),
            (
                "/workspace/examples/llm/tensorrtllm/operators/triton_core_models/mock",
                "tensorrt_llm",
            ),
        ],
        "template_arguments": {
            "tokenizer_dir": "{args.hf_download}",
            "triton_max_batch_size": "{args.max_batch_size}",
            "preprocessing_instance_count": "{args.preprocessing_instance_count}",
            "postprocessing_instance_count": "{args.postprocessing_instance_count}",
            "context_token_latency_ms": "0.1",
            "generate_token_latency_ms": "0.5",
        },
    },
    "llama-3.1-70b-instruct": {
        "hf_id": "meta-llama/Meta-Llama-3.1-70B-Instruct",
        "download_model_name": "llama-3.1-70b-instruct",
        "convert": [
            "quantization/quantize.py",
            "--dtype",
            "float16",
            "--qformat",
            "fp8",
            "--calib_size",
            "512",
            "--kv_cache_dtype",
            "fp8",
        ],
        "build": [
            "--gpt_attention_plugin",
            "float16",
            "--max_seq_len",
            "131072",
            "--use_fused_mlp",
            "enable",
            "--reduce_fusion",
            "disable",
            "--multiple_profiles",
            "enable",
            "--use_paged_context_fmha",
            "enable",
        ],
        "max_num_tokens": 2048,
        "max_batch_size": 512,
        "templates": [
            "preprocessing",
            "postprocessing",
            "ensemble",
            ("tensorrt_llm", "context"),
            ("tensorrt_llm", "generate"),
            "tensorrt_llm",
        ],
        "template_arguments": {
            "triton_max_batch_size": "{args.max_batch_size}",
            "decoupled_mode": "True",
            "preprocessing_instance_count": "{args.preprocessing_instance_count}",
            "postprocessing_instance_count": "{args.postprocessing_instance_count}",
            "triton_backend": "tensorrtllm",
            "enable_chunked_context": "{args.enable_chunked_context}",
            "max_beam_width": "1",
            "engine_dir": "{args.tensorrtllm_engine}",
            "exclude_input_in_output": "True",
            "enable_kv_cache_reuse": "False",
            "batching_strategy": "inflight_fused_batching",
            "max_queue_delay_microseconds": "0",
            "max_queue_size": "0",
            "participant_ids": "{args.participant_ids}",
            "tokenizer_dir": "{args.hf_download}",
            "encoder_input_features_data_type": "TYPE_FP16",
        },
    },
    "llama-3.1-8b-instruct": {
        "hf_id": "meta-llama/Meta-Llama-3.1-8B-Instruct",
        "download_model_name": "llama-3.1-8b-instruct",
        "convert": ["llama/convert_checkpoint.py", "--dtype", "float16"],
        "build": [
            "--remove_input_padding",
            "enable",
            "--gpt_attention_plugin",
            "float16",
            "--context_fmha",
            "enable",
            "--gemm_plugin",
            "float16",
            "--paged_kv_cache",
            "enable",
        ],
        "max_num_tokens": 16384,
        "max_batch_size": 64,
        "templates": [
            "preprocessing",
            "postprocessing",
            "ensemble",
            ("tensorrt_llm", "context"),
            ("tensorrt_llm", "generate"),
            "tensorrt_llm",
        ],
        "template_arguments": {
            "triton_max_batch_size": "{args.max_batch_size}",
            "decoupled_mode": "True",
            "preprocessing_instance_count": "{args.preprocessing_instance_count}",
            "postprocessing_instance_count": "{args.postprocessing_instance_count}",
            "triton_backend": "tensorrtllm",
            "max_beam_width": "1",
            "engine_dir": "{args.tensorrtllm_engine}",
            "exclude_input_in_output": "True",
            "enable_kv_cache_reuse": "False",
            "batching_strategy": "inflight_fused_batching",
            "max_queue_delay_microseconds": "0",
            "max_queue_size": "0",
            "participant_ids": "0",
            "tokenizer_dir": "{args.hf_download}",
            "encoder_input_features_data_type": "TYPE_FP16",
        },
    },
    "llama-3-8b-instruct-generate": {
        "hf_id": "meta-llama/Meta-Llama-3-8B-Instruct",
        "max_batch_size": 256,
        "model_repo_name": "llama-3-8b-instruct-disaggregated",
        "download_model_name": "llama-3-8b-instruct",
        "convert": [
            "quantization/quantize.py",
            "--dtype",
            "float16",
            "--qformat",
            "fp8",
            "--calib_size",
            "512",
            "--kv_cache_dtype",
            "fp8",
        ],
        "build": [
            "--gpt_attention_plugin",
            "float16",
            "--workers",
            "{args.tp_size}",
            "--max_seq_len",
            "1024",
            "--use_fused_mlp",
            "enable",
            "--multiple_profiles",
            "enable",
        ],
        "max_num_tokens": 256,
        "templates": [
            ("tensorrt_llm", "generate"),
            "postprocessing",
        ],
        "template_arguments": {
            "triton_max_batch_size": "{args.max_batch_size}",
            "decoupled_mode": "True",
            "preprocessing_instance_count": "{args.preprocessing_instance_count}",
            "postprocessing_instance_count": "{args.postprocessing_instance_count}",
            "triton_backend": "tensorrtllm",
            "max_beam_width": "1",
            "engine_dir": "{args.tensorrtllm_engine}",
            "exclude_input_in_output": "True",
            "enable_kv_cache_reuse": "False",
            "batching_strategy": "inflight_fused_batching",
            "max_queue_delay_microseconds": "0",
            "max_queue_size": "0",
            "participant_ids": "0",
            "tokenizer_dir": "{args.hf_download}",
            "encoder_input_features_data_type": "TYPE_FP16",
        },
    },
    "llama-3-8b-instruct-context": {
        "hf_id": "meta-llama/Meta-Llama-3-8B-Instruct",
        "max_batch_size": 256,
        "model_repo_name": "llama-3-8b-instruct-disaggregated",
        "download_model_name": "llama-3-8b-instruct",
        "convert": [
            "quantization/quantize.py",
            "--dtype",
            "float16",
            "--qformat",
            "fp8",
            "--calib_size",
            "512",
            "--kv_cache_dtype",
            "fp8",
        ],
        "build": [
            "--gpt_attention_plugin",
            "float16",
            "--workers",
            "{args.tp_size}",
            "--max_seq_len",
            "8192",
            "--use_fused_mlp",
            "enable",
            "--multiple_profiles",
            "enable",
        ],
        "max_num_tokens": 8192,
        "templates": [
            "/workspace/examples/disaggregated_serving/tensorrtllm_templates/context",
            "preprocessing",
        ],
        "template_arguments": {
            "triton_max_batch_size": "{args.max_batch_size}",
            "decoupled_mode": "False",
            "preprocessing_instance_count": "{args.preprocessing_instance_count}",
            "postprocessing_instance_count": "{args.postprocessing_instance_count}",
            "triton_backend": "tensorrtllm",
            "max_beam_width": "1",
            "engine_dir": "{args.tensorrtllm_engine}",
            "exclude_input_in_output": "True",
            "enable_kv_cache_reuse": "False",
            "batching_strategy": "inflight_fused_batching",
            "max_queue_delay_microseconds": "0",
            "max_queue_size": "0",
            "participant_ids": "0",
            "tokenizer_dir": "{args.hf_download}",
            "encoder_input_features_data_type": "TYPE_FP16",
        },
    },
    "llama-3-8b-instruct": {
        "hf_id": "meta-llama/Meta-Llama-3-8B-Instruct",
        "convert": [
            "quantization/quantize.py",
            "--dtype",
            "float16",
            "--qformat",
            "fp8",
            "--calib_size",
            "512",
            "--kv_cache_dtype",
            "fp8",
        ],
        "build": [
            "--gpt_attention_plugin",
            "float16",
            "--workers",
            "{args.tp_size}",
            "--max_seq_len",
            "8192",
            "--use_fused_mlp",
            "enable",
            "--multiple_profiles",
            "enable",
            "--reduce_fusion",
            "{args.reduce_fusion}",
        ],
        "max_num_tokens": 16384,
        "max_batch_size": 512,
        "templates": [
            "preprocessing",
            "postprocessing",
            "ensemble",
            ("tensorrt_llm", "context"),
            ("tensorrt_llm", "generate"),
            "tensorrt_llm",
        ],
        "template_arguments": {
            "triton_max_batch_size": "{args.max_batch_size}",
            "decoupled_mode": "True",
            "preprocessing_instance_count": "{args.preprocessing_instance_count}",
            "postprocessing_instance_count": "{args.postprocessing_instance_count}",
            "triton_backend": "tensorrtllm",
            "max_beam_width": "1",
            "engine_dir": "{args.tensorrtllm_engine}",
            "exclude_input_in_output": "True",
            "enable_kv_cache_reuse": "False",
            "batching_strategy": "inflight_fused_batching",
            "max_queue_delay_microseconds": "0",
            "max_queue_size": "0",
            "participant_ids": "0",
            "tokenizer_dir": "{args.hf_download}",
            "encoder_input_features_data_type": "TYPE_FP16",
        },
    },
    "llama-3-8b-instruct-default": {
        "hf_id": "meta-llama/Meta-Llama-3-8B-Instruct",
        "download_model_name": "llama-3-8b-instruct",
        "convert": ["llama/convert_checkpoint.py", "--dtype", "float16"],
        "build": [
            "--remove_input_padding",
            "enable",
            "--gpt_attention_plugin",
            "float16",
            "--context_fmha",
            "enable",
            "--gemm_plugin",
            "float16",
            "--paged_kv_cache",
            "enable",
        ],
        "max_batch_size": 64,
        "templates": [
            "preprocessing",
            "postprocessing",
            "ensemble",
            ("tensorrt_llm", "context"),
            ("tensorrt_llm", "generate"),
            "tensorrt_llm",
        ],
        "template_arguments": {
            "triton_max_batch_size": "{args.max_batch_size}",
            "decoupled_mode": "True",
            "preprocessing_instance_count": "{args.preprocessing_instance_count}",
            "postprocessing_instance_count": "{args.postprocessing_instance_count}",
            "triton_backend": "tensorrtllm",
            "max_beam_width": "1",
            "engine_dir": "{args.tensorrtllm_engine}",
            "exclude_input_in_output": "True",
            "enable_kv_cache_reuse": "False",
            "batching_strategy": "inflight_fused_batching",
            "max_queue_delay_microseconds": "0",
            "max_queue_size": "0",
            "participant_ids": "0",
            "tokenizer_dir": "{args.hf_download}",
            "encoder_input_features_data_type": "TYPE_FP16",
        },
    },
    "llama-3-70b-instruct-context": {
        "hf_id": "meta-llama/Meta-Llama-3-70B-Instruct",
        "download_model_name": "llama-3-70b-instruct",
        "model_repo_name": "llama-3-70b-disaggegated",
        "max_batch_size": 128,
        "convert": [
            "quantization/quantize.py",
            "--dtype",
            "float16",
            "--qformat",
            "fp8",
            "--calib_size",
            "512",
            "--kv_cache_dtype",
            "fp8",
        ],
        "build": [
            "--gpt_attention_plugin",
            "float16",
            "--workers",
            "{args.tp_size}",
            "--max_seq_len",
            "8192",
            "--use_fused_mlp",
            "enable",
            "--reduce_fusion",
            "{args.reduce_fusion}",
            "--multiple_profiles",
            "enable",
        ],
        "max_num_tokens": 8192,
        "templates": [
            "preprocessing",
            "/workspace/examples/disaggregated_serving/tensorrtllm_templates/context",
        ],
        "template_arguments": {
            "triton_max_batch_size": "{args.max_batch_size}",
            "decoupled_mode": "True",
            "preprocessing_instance_count": "{args.preprocessing_instance_count}",
            "postprocessing_instance_count": "{args.postprocessing_instance_count}",
            "triton_backend": "tensorrtllm",
            "max_beam_width": "1",
            "engine_dir": "{args.tensorrtllm_engine}",
            "exclude_input_in_output": "True",
            "enable_kv_cache_reuse": "False",
            "batching_strategy": "inflight_fused_batching",
            "max_queue_delay_microseconds": "0",
            "max_queue_size": "0",
            "participant_ids": "{args.participant_ids}",
            "tokenizer_dir": "{args.hf_download}",
            "encoder_input_features_data_type": "TYPE_FP16",
        },
    },
    "llama-3-70b-instruct-generate": {
        "hf_id": "meta-llama/Meta-Llama-3-70B-Instruct",
        "download_model_name": "llama-3-70b-instruct",
        "model_repo_name": "llama-3-70b-disaggegated",
        "max_batch_size": 128,
        "convert": [
            "quantization/quantize.py",
            "--dtype",
            "float16",
            "--qformat",
            "fp8",
            "--calib_size",
            "512",
            "--kv_cache_dtype",
            "fp8",
        ],
        "build": [
            "--gpt_attention_plugin",
            "float16",
            "--workers",
            "{args.tp_size}",
            "--max_seq_len",
            "1024",
            "--use_fused_mlp",
            "enable",
            "--reduce_fusion",
            "{args.reduce_fusion}",
            "--multiple_profiles",
            "enable",
        ],
        "max_num_tokens": 128,
        "templates": [
            "postprocessing",
            "/workspace/examples/disaggregated_serving/tensorrtllm_templates/generate",
        ],
        "template_arguments": {
            "triton_max_batch_size": "{args.max_batch_size}",
            "decoupled_mode": "True",
            "preprocessing_instance_count": "{args.preprocessing_instance_count}",
            "postprocessing_instance_count": "{args.postprocessing_instance_count}",
            "triton_backend": "tensorrtllm",
            "max_beam_width": "1",
            "engine_dir": "{args.tensorrtllm_engine}",
            "exclude_input_in_output": "True",
            "enable_kv_cache_reuse": "False",
            "batching_strategy": "inflight_fused_batching",
            "max_queue_delay_microseconds": "0",
            "max_queue_size": "0",
            "participant_ids": "{args.participant_ids}",
            "tokenizer_dir": "{args.hf_download}",
            "encoder_input_features_data_type": "TYPE_FP16",
        },
    },
    "llama-3-70b-instruct": {
        "hf_id": "meta-llama/Meta-Llama-3-70B-Instruct",
        "max_batch_size": 512,
        "convert": [
            "quantization/quantize.py",
            "--dtype",
            "float16",
            "--qformat",
            "fp8",
            "--calib_size",
            "512",
            "--kv_cache_dtype",
            "fp8",
        ],
        "build": [
            "--gpt_attention_plugin",
            "float16",
            "--workers",
            "{args.tp_size}",
            "--max_seq_len",
            "8192",
            "--use_fused_mlp",
            "enable",
            "--reduce_fusion",
            "{args.reduce_fusion}",
            "--multiple_profiles",
            "enable",
        ],
        "max_num_tokens": 16384,
        "templates": [
            "preprocessing",
            "postprocessing",
            "ensemble",
            "tensorrt_llm",
        ],
        "template_arguments": {
            "triton_max_batch_size": "{args.max_batch_size}",
            "decoupled_mode": "True",
            "preprocessing_instance_count": "{args.preprocessing_instance_count}",
            "postprocessing_instance_count": "{args.postprocessing_instance_count}",
            "triton_backend": "tensorrtllm",
            "max_beam_width": "1",
            "engine_dir": "{args.tensorrtllm_engine}",
            "exclude_input_in_output": "True",
            "enable_kv_cache_reuse": "False",
            "batching_strategy": "inflight_fused_batching",
            "max_queue_delay_microseconds": "0",
            "max_queue_size": "0",
            "participant_ids": "{args.participant_ids}",
            "tokenizer_dir": "{args.hf_download}",
            "encoder_input_features_data_type": "TYPE_FP16",
        },
    },
}
